{
  "timestamp": "2025-08-13T10:18:10.784724",
  "total_time_seconds": 17.169411420822144,
  "total_tests": 3,
  "passed_tests": 2,
  "failed_tests": 1,
  "success_rate": 0.6666666666666666,
  "system_ready": true,
  "gpu_count": 3,
  "pytorch_version": "2.7.1+cu118",
  "test_results": {
    "test_system.py": {
      "description": "System Tests (GPU Detection, Config Loading, Basic Functionality)",
      "success": true,
      "returncode": 0,
      "stdout": "\ud83e\uddea Starting uTooth Sweep System Tests\n==================================================\n\n\ud83d\udd0d Running test_gpu_detection...\n\u2705 test_gpu_detection: PASSED\n\n\ud83d\udd0d Running test_pytorch_cuda_setup...\n\u2705 test_pytorch_cuda_setup: PASSED\n\n\ud83d\udd0d Running test_config_loading...\n\u2705 test_config_loading: PASSED\n\n\ud83d\udd0d Running test_directory_creation...\n\u2705 test_directory_creation: PASSED\n\n\ud83d\udd0d Running test_optuna_setup...\n\u2705 test_optuna_setup: PASSED\n\n\ud83d\udd0d Running test_hyperparameter_suggestion...\n\u2705 test_hyperparameter_suggestion: PASSED\n\n\ud83d\udd0d Running test_monitoring_setup...\n\u2705 test_monitoring_setup: PASSED\n\n\ud83d\udd0d Running test_mock_sweep...\n\u2705 test_mock_sweep: PASSED\n\n======================================================================\n\ud83e\uddea SYSTEM TEST SUMMARY\n======================================================================\nTotal Tests: 8\nPassed: 8 \u2705\nFailed: 0 \u274c\nSuccess Rate: 100.0%\n\n\ud83d\udda5\ufe0f  GPU Information:\n   CUDA Available: True\n   GPUs Detected: 3\n   GPU 0: NVIDIA GeForce RTX 3090 (23.7GB)\n   GPU 1: NVIDIA GeForce RTX 3090 (23.7GB)\n   GPU 2: NVIDIA GeForce RTX 3090 (23.7GB)\n======================================================================\n\n\ud83d\udcc1 Test results saved to: /home/user/utooth/scripts/tests/test_results.json\n",
      "stderr": "[I 2025-08-13 10:17:55,281] A new study created in RDB with name: utooth_default_sweep\n[I 2025-08-13 10:17:55,390] A new study created in RDB with name: utooth_default_sweep\n[I 2025-08-13 10:17:55,450] Trial 0 finished with value: 0.5684494720793793 and parameters: {'learning_rate': 0.00043284502212938834, 'loss_alpha': 0.7, 'loss_gamma': 1.75, 'batch_size': 4, 'start_filters': 16, 'n_blocks': 4, 'normalization': 'batch', 'activation': 'leaky', 'attention': False}. Best is trial 0 with value: 0.5684494720793793.\n[I 2025-08-13 10:17:55,500] Trial 1 finished with value: 0.7259364937873714 and parameters: {'learning_rate': 0.00017258215396625024, 'loss_alpha': 0.4, 'loss_gamma': 1.25, 'batch_size': 5, 'start_filters': 64, 'n_blocks': 5, 'normalization': 'batch', 'activation': 'leaky', 'attention': False}. Best is trial 0 with value: 0.5684494720793793.\n[I 2025-08-13 10:17:55,552] Trial 2 finished with value: 0.7765614875355391 and parameters: {'learning_rate': 0.00011439974749291271, 'loss_alpha': 0.7, 'loss_gamma': 1.0, 'batch_size': 4, 'start_filters': 32, 'n_blocks': 3, 'normalization': 'batch', 'activation': 'silu', 'attention': False}. Best is trial 0 with value: 0.5684494720793793.\n",
      "execution_time": 2.294113874435425
    },
    "test_mini_sweep.py": {
      "description": "Mini Sweep Integration Test (End-to-End Components)",
      "success": true,
      "returncode": 0,
      "stdout": "\ud83d\ude80 Mini Sweep Integration Test\n======================================================================\n\ud83e\uddea Running Mini Sweep Test\n==================================================\n\ud83d\udccb Creating test configuration...\n\ud83d\udcc1 Creating mock data directory...\n\ud83d\udccb Config file: /tmp/tmpk35e0nzr/test_config.yaml\n\ud83d\udcc1 Data directory: /tmp/tmpk35e0nzr/mock_data\n\ud83d\udda5\ufe0f  Available GPUs: 3\n\n\ud83d\udd27 Testing configuration loading...\n\u2705 Configuration loaded and valid\n\n\ud83d\udcc2 Testing directory creation...\n\u2705 Sweep directory created: /tmp/tmpk35e0nzr/test_mini_sweep_101756_20250813_101756\n\n\ud83d\udd0d Testing Optuna study creation...\n\u2705 Optuna study created: test_mini_sweep_101756\n\n\u2699\ufe0f  Testing hyperparameter suggestion...\n\u2705 Parameters suggested: ['activation', 'attention', 'batch_size', 'learning_rate', 'loss_alpha', 'loss_gamma', 'n_blocks', 'normalization', 'start_filters']\n\n\ud83d\udcca Testing monitoring setup...\n\u2705 Monitor created with plots dir: True\n\n==================================================\n\u2705 Mini Sweep Test: ALL COMPONENTS WORKING\n==================================================\n\ud83d\udcc1 Test results saved to: /home/user/utooth/scripts/tests/mini_sweep_test_results.json\n\n\ud83e\uddea Testing Sweep Runner Import and Help\n--------------------------------------------------\n\u2705 SweepRunner imported successfully\n\u2705 Help command works\n\n======================================================================\n\ud83e\uddea MINI SWEEP TEST SUMMARY\n======================================================================\n\u2705 ALL TESTS PASSED\n\u2705 Sweep system is ready for use!\n\u2705 All 3 GPUs detected and available\n\u2705 Configuration system working\n\u2705 Optuna integration working\n\u2705 Monitoring system working\n\n\ud83d\ude80 Ready to run full hyperparameter sweeps!\n======================================================================\n",
      "stderr": "[I 2025-08-13 10:17:56,786] A new study created in RDB with name: test_mini_sweep_101756\n",
      "execution_time": 7.167395114898682
    },
    "test_gpu_utilization.py": {
      "description": "GPU Utilization Test (Memory and Compute)",
      "success": false,
      "returncode": 1,
      "stdout": "\ud83d\ude80 GPU Utilization Test Suite\n==================================================\nPyTorch version: 2.7.1+cu118\nCUDA available: True\nGPU count: 3\nGPU 0: NVIDIA GeForce RTX 3090 (23.7GB)\nGPU 1: NVIDIA GeForce RTX 3090 (23.7GB)\nGPU 2: NVIDIA GeForce RTX 3090 (23.7GB)\n\nRunning Single GPU Test...\n\ud83e\uddea Testing single GPU...\n\ud83d\udd25 Starting GPU 0 worker...\n\u2705 GPU 0 worker completed\n\u2705 Single GPU Test completed\n\nRunning Multi-GPU Sequential Test...\n\ud83e\uddea Testing multi-GPU sequential...\n\ud83d\udd25 Starting GPU 0 worker...\n\u2705 GPU 0 worker completed\n\ud83d\udd25 Starting GPU 1 worker...\n\u2705 GPU 1 worker completed\n\ud83d\udd25 Starting GPU 2 worker...\n\u2705 GPU 2 worker completed\n\u2705 Multi-GPU Sequential Test completed\n\nRunning Multi-GPU Parallel Test...\n\ud83e\uddea Testing multi-GPU parallel...\n   Testing GPUs: [0, 1, 2]\n\ud83d\udd25 Starting GPU 0 worker...\n\ud83d\udd25 Starting GPU 2 worker...\n\ud83d\udd25 Starting GPU 1 worker...\n\u2705 Multi-GPU Parallel Test completed\n\nRunning GPU Memory Stress Test...\n\ud83e\uddea Testing GPU memory stress...\n\u2705 GPU Memory Stress Test completed\n\n======================================================================\n\ud83e\uddea GPU UTILIZATION TEST SUMMARY\n======================================================================\n\u274c Multi-GPU Parallel Test: FAILED\n     GPU 0: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n     GPU 1: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n     GPU 2: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\n\ud83d\udcca Memory Test Results:\n     GPU 0: 20.0GB max allocation\n     GPU 1: 20.0GB max allocation\n     GPU 2: 20.0GB max allocation\n======================================================================\n\ud83d\udcc1 Results saved to: /home/user/utooth/scripts/tests/gpu_utilization_results.json\n",
      "stderr": "",
      "execution_time": 17.169395446777344
    }
  }
}