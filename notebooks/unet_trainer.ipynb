{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nfrom src.data.volume_dataloader import CTScanDataModule, CTDataSet\nfrom src.models.unet import UNet\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torch import sigmoid, where, cuda\nimport src.utils.ct_utils as ct_utils\n\nDATA_PATH = '/home/gaetano/utooth/DATA/'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "dataset = CTScanDataModule(DATA_PATH, batch_size=5)\n\nmodel = UNet(in_channels=1,\n             out_channels=4,\n             n_blocks=4,\n             start_filters=32,\n             activation='relu',\n             normalization='batch',\n             conv_mode='same',\n             dim=3,\n             attention=False,\n             loss_alpha=.5236,  # .7\n             loss_gamma=1.0, # 3/4\n             learning_rate=2e-3)\n\nlr_monitor = LearningRateMonitor(logging_interval='epoch')\ncheckpoint = ModelCheckpoint(monitor='val_loss', dirpath='../outputs/checkpoints')\nwandb_logger = WandbLogger(save_dir='../outputs/logs')\ntrainer = Trainer(accelerator='gpu',\n                  devices=1,\n                  strategy='auto',\n                  log_every_n_steps=1,\n                    max_epochs=2,\n                #   max_epochs=50,\n                  callbacks=[checkpoint],\n                  logger=wandb_logger,\n                  default_root_dir='../outputs')"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaetano/utooth/utooth_env/lib/python3.10/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 Ti Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gaetano/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mplayweird\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gaetano/utooth/wandb/run-20250720_150221-olkhuk2i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/playweird/utooth/runs/olkhuk2i' target=\"_blank\">daily-microwave-5</a></strong> to <a href='https://wandb.ai/playweird/utooth' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/playweird/utooth' target=\"_blank\">https://wandb.ai/playweird/utooth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/playweird/utooth/runs/olkhuk2i' target=\"_blank\">https://wandb.ai/playweird/utooth/runs/olkhuk2i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | down_convs | ModuleList | 3.5 M \n",
      "1 | up_convs   | ModuleList | 2.1 M \n",
      "2 | conv_final | Conv3d     | 132   \n",
      "------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.412    Total estimated model params size (MB)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The project_name method is deprecated and will be removed in a future release. Please use `run.project` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# checkpoint.save_checkpoint(trainer)\u001b[39;00m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1311\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar_callback\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m-> 1311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1371\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# reload dataloaders\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_evaluation_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:170\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mreset_test_dataloader()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_should_reload_val_dl:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_val_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:551\u001b[0m, in \u001b[0;36mTrainerDataLoadingMixin.reset_val_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    549\u001b[0m has_step \u001b[38;5;241m=\u001b[39m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mis_defined() \u001b[38;5;129;01mand\u001b[39;00m has_step:\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mRunningStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALIDATING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl_module\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# store epoch of dataloader reset for reload_dataloaders_every_n_epochs\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_val_dl_reload_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/data_loading.py:508\u001b[0m, in \u001b[0;36mTrainerDataLoadingMixin._reset_eval_dataloader\u001b[0;34m(self, mode, model)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataloaders) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloaders):\n\u001b[1;32m    506\u001b[0m         orig_num_batches \u001b[38;5;241m=\u001b[39m num_batches \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    507\u001b[0m             \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m--> 508\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    510\u001b[0m         )\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_check(dataloader, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;241m.\u001b[39mdataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dataloader \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# percent or num_steps\u001b[39;00m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:114\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, training_type, model)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if a given Dataloader has ``__len__`` method implemented i.e. if it is a finite dataloader or\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03minfinite dataloader.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m        If the length of Dataloader is 0, as it requires at least one batch\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     total_length \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     local_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/dp.py:82\u001b[0m, in \u001b[0;36mDataParallelPlugin.reduce\u001b[0;34m(self, collection, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     original_dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mto(original_dtype)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py:96\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/utooth/utooth_env/lib/python3.10/site-packages/pytorch_lightning/plugins/training_type/dp.py:80\u001b[0m, in \u001b[0;36mDataParallelPlugin.reduce.<locals>.mean\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmean\u001b[39m(t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     79\u001b[0m     original_dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mto(original_dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# cuda.empty_cache()\n",
    "trainer.fit(model, dataset)\n",
    "# checkpoint.save_checkpoint(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5659d932970f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mct_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "check_model_path = checkpoint.best_model_path\n",
    "model.eval()\n",
    "x, y = dataset.ct_val[0]\n",
    "y_hat = sigmoid(model(x.unsqueeze(0)))\n",
    "x = x.squeeze(0).numpy()\n",
    "y_hat = y_hat.squeeze(0)\n",
    "y_hat = where(y_hat > 0.5, 1, 0).numpy()\n",
    "y_hat = y_hat.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "%matplotlib tk\nct_utils.plot_3d_with_labels(x, y_hat, threshold=1200, transpose=[2, 1, 0], step_size=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "# trainer.tune(model, dataset)\n# Run learning rate finder - Updated for PyTorch Lightning 2.0+\nfrom pytorch_lightning.tuner import Tuner\n\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(model, dataset)\n\n# Results can be found in\nlr_finder.results\n\n# Plot with\nfig = lr_finder.plot(suggest=True)\nfig.show()\n\n# Pick point based on plot, or get suggestion\nnew_lr = lr_finder.suggestion()\n\n# update hparams of the model\nmodel.hparams.learning_rate = new_lr"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utooth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}